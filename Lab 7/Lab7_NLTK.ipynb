{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Giới thiệu về thư viện NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Command 'Downloader> l' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Command 'Downloader> l' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Command 'Downloader> d' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] tagsets_json........ Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet2022......... Open English Wordnet 2022\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package gutenberg to\n",
      "        C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "      Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk # import thư viện NLTK\n",
    "nltk.download_shell() # download shell của NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg') # tải về gói dữ liệu Gutenberg\n",
    "\n",
    "gb = nltk.corpus.gutenberg # tạo biến gb truy cập vào nội dung Gutenberg\n",
    "print(\"Gutenberg files : \", gb.fileids()) # xem tên các tập tin có trong gói 'gutenberg' bằng lệnh fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt') # truy cập nội dung tập tin\n",
    "len(macbeth) # chiều dài của văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth [:10] # hiển thị 10 từ đầu tiên của tập tin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # tải về gói dữ liệu punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Macbeth',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1603',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Thunder', 'and', 'Lightning', '.'],\n",
       " ['Enter', 'three', 'Witches', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt') # Trích xuất câu từ file văn bản và lưu trữ vào biến macbeth_sents\n",
    "macbeth_sents[:5] #  trích 5 câu đầu tiên của tập tin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tìm 1 từ với NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(macbeth) # Tạo đối tượng Text từ danh sách trong văn bản \"Macbeth\", lưu trữ vào biến text\n",
    "text.concordance('Stage') # Tìm từ ‘Stage’ xuất hiện trong văn bản text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_. bloody_: the_,\n"
     ]
    }
   ],
   "source": [
    "text.common_contexts(['Stage']) # Tìm từ xuất hiện trước và sau từ ‘Stage’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n"
     ]
    }
   ],
   "source": [
    "# Tìm từ tương tự từ ‘Stage’\n",
    "text.similar('Stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Phân tích tần số của các từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(macbeth) # Tạo đối tượng FreqDist từ danh sách trong văn bản \"Macbeth\"\n",
    "fd.most_common(10) # Lấy ra 10 từ xuất hiện nhiều nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Tải về gói dữ liệu stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = nltk.corpus.stopwords.words('english') # Tạo danh sách sw chứa các từ dừng trong tiếng Anh\n",
    "print(len(sw)) # Hiển thị số lượng từ dừng\n",
    "list(sw)[:10] # Lấy 10 từ dừng đầu tiên "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14946"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw] \n",
    "# Tạo danh sách macbeth_filtered chứa các từ trong \"Macbeth\" nhưng loại bỏ các từ dừng\n",
    "len(macbeth_filtered) # Hiển thị số lượng từ trong danh sách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(macbeth_filtered) # Tạo đối tượng FreqDist từ danh sách trong văn bản \"macbeth_filtered\"\n",
    "fd.most_common(10) # Lấy ra 10 từ xuất hiện nhiều nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 90),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('vpon', 62),\n",
       " ('thee', 61),\n",
       " ('macd', 58),\n",
       " ('vs', 57)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # Import thư viện string\n",
    "punctuation = set(string.punctuation) # Tạo tập hợp punctuation chứa các ký tự dấu câu\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation] # Tạo danh sách macbeth_filtered2 chứa các từ trong \"Macbeth\" nhưng loại bỏ cả từ dừng và dấu câu, tất cả chữ đều chuyển về dạng chữ thường\n",
    "fd = nltk.FreqDist(macbeth_filtered2) # Tạo đối tượng FreqDist từ danh sách trong văn bản \"macbeth_filtered2\"\n",
    "fd.most_common(10) # Lấy ra 10 từ xuất hiện nhiều nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lựa chọn các từ trong văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Chamberlaines',\n",
       " 'Distinguishes',\n",
       " 'Gallowgrosses',\n",
       " 'Metaphysicall',\n",
       " 'Northumberland',\n",
       " 'Voluptuousnesse',\n",
       " 'commendations',\n",
       " 'multitudinous',\n",
       " 'supernaturall',\n",
       " 'vnaccompanied']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_words = [w for w in macbeth if len(w)> 12] # Tạo danh sách long_words chứa các từ trong \"Macbeth\" có độ dài lớn hơn 12 ký tự\n",
    "sorted(long_words) # Sắp xếp danh sách theo thứ tự chữ cái"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious_words = [w for w in macbeth if 'ious' in w] # Tạo danh sách ious_words chứa các từ trong \"Macbeth\" có chuỗi \"ious\" bên trong\n",
    "ious_words = set(ious_words) # Chuyển đổi danh sách thành một tập hợp loại bỏ các từ trùng lặp\n",
    "sorted(ious_words) # Sắp xếp danh sách theo thứ tự chữ cái"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Bigrams và collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2)) # Tạo đối tượng FreqDist từ các bigram trong danh sách macbeth_filtered2.\n",
    "bgrms.most_common(15) # Lấy ra 15 bigram xuất hiện nhiều nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2)) # Tạo đối tượng FreqDist từ các trigram trong danh sách macbeth_filtered2\n",
    "tgrms.most_common(10) # Lấy ra 10 trigram xuất hiện nhiều nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Sử dụng văn bản trên mạng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request # Từ thư viện urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\" # Gán biến ulr chứa  địa chỉ url\n",
    "response = request.urlopen(url) # Mở kết nối đến URL và lưu trữ phản hồi vào biến response\n",
    "raw = response.read().decode('utf8') # Đọc nội dung và giải mã bằng định dạng UTF-8, lưu trữ vào biến raw\n",
    "raw[:75] # Lấy 75 ký tự đầu tiên từ chuỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request # Từ thư viện urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\" # Gán biến ulr chứa  địa chỉ url\n",
    "response = request.urlopen(url) # Mở kết nối đến URL và lưu trữ phản hồi vào biến response\n",
    "raw = response.read().decode('utf-8-sig') # Đọc nội dung và giải mã bằng định dạng UTF-8 có BOM (Byte Order Mark), lưu trữ vào biến raw\n",
    "print(raw[:75]) # Lấy 75 ký tự đầu tiên từ chuỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize (raw) # Tách chuỗi raw thành các token và lưu trữ vào biến tokens\n",
    "webtext = nltk.Text(tokens) # Tạo  đối tượng Text từ danh sách tokens, lưu trữ vào biến webtext.\n",
    "webtext[:12] # Lấy 12 token đầu tiên từ đối tượng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Rút trích văn bản từ trang html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\" # Gán biến ulr chứa  địa chỉ url\n",
    "html = request.urlopen(url).read().decode('utf8') # Mở kết nối, đọc nội dung HTML và giải mã bằng định dạng UTF-8, lưu trữ vào biến html\n",
    "html[:120] # Lấy 120 ký tự đầu tiên từ chuỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Import thư viện BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"html.parser\").get_text()  # Tạo đối tượng BeautifulSoup từ chuỗi html, sử dụng cú pháp 'html.parser', lấy nội dung văn bản và lưu trữ vào biến raw.\n",
    "tokens = nltk.word_tokenize(raw) # Tách chuỗi raw thành các token và lưu trữ vào biến tokens\n",
    "text = nltk.Text(tokens) # Tạo đối tượng Text từ danh sách tokens, lưu trữ vào biến text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Phân tích cảm xúc người dùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews') # Tải về gói dữ liệu movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # Import thư viện random\n",
    "reviews = nltk.corpus.movie_reviews # Truy cập vào kho dữ liệu movie_reviews\n",
    "documents = [(list(reviews.words(fileid)), category) # Tạo danh sách documents chứa các cặp (list các từ trong bài đánh giá), thể loại có thể là 'pos' (tích cực) hoặc 'neg' (tiêu cực).\n",
    "for category in reviews.categories()\n",
    "for fileid in reviews.fileids(category)]\n",
    "random.shuffle(documents) # Trộn ngẫu nhiên các cặp trong danh sách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredients : down - on - his - luck evangelist , church synopsis : sonny dewey ( robert duvall ) is a tireless texas pentecostal preacher who unexpectedly catches his wife ( farrah fawcett ) in bed with another guy . in a regrettable crime of passion he takes a baseball bat to the guy ' s head , and suddenly finds himself a fugitive for murder , and estranged from his wife and two kids . to atone for his sins , sonny flees to a rural bayou town in louisiana and baptizes himself as a new creature - - the apostle e . f . as the apostle e . f . sonny ' s main mission is to revive an abandoned church community and preach the gospel at the local radio station . soon he sets about resurrecting the one way road to heaven holiness temple . but sonny knows his time is short ; one day the police will find him . opinion : this movie is a thought - provoking look at an evangelist in real life terms . i believe that robert duvall ( who is the producer , director , writer , and main star of the apostle ) deserves an oscar for his performance as sonny the religious crusader - - a performance which is so complex and realistic it ranks as one of the finest acting performances on film . duvall ' s portrayal of a true believer is authentic , superb , and inspired ; it offers the audience a completely honest look at southern religion , as well as a portrait of a fallible , complicated man driven by his beliefs . incidentally , in real life duvall was just as ' driven ' in getting the project underway . since no major hollywood studio wanted to risk financing a complex movie about an evangelist , the apostle as a project lay dormant for almost 13 years before duvall was able to get it done , paying for its production with his own money ( about five million dollars ) . in terms of hard - hitting realism and character portrayal the apostle would rank as one of the best movies of this decade , and i emphatically recommend the apostle for connoisseurs of stage and fine acting on film . similarly , those with a steep background in pentecostal christianity would find the apostle a thought - provoking experience . on the other hand , the apostle ' s main strength - - duvall ' s preaching - - is also what limits the apostle ' s target audience . in terms of plot and pure entertainment value , the apostle is definitely not for everyone . teenagers , action fans , comedy fans , and people who find church completely boring should stay far away from this movie , since three quarters of the film is watching sonny preach at church . in other words , the apostle is a four star performance , but with a very limited audience .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_review = ' '.join(documents[0][0]) # Lấy bài đánh giá đầu tiên trong danh sách documents, kết hợp các từ lại thành chuỗi duy nhất và lưu trữ vào biến first_review.\n",
    "print(first_review) # Hiển thị nội dung của bài đánh giá đầu tiên\n",
    "\n",
    "documents[0][1] # Lấy và hiển thị thể loại của bài đánh giá đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words()) # Tạo đối tượng FreqDist để đếm tần suất xuất hiện của tất cả các từ, tất cả từ đều được chuyển về dạng chữ thường.\n",
    "word_features = list(all_words) # Tạo danh sách word_features chứa tất cả các từ duy nhất từ all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def document_features(document, word_features): # Gọi hàm document_features\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words) \n",
    "        # Tạo một từ điển features với các từ là khóa và giá trị là True/False tùy thuộc vào việc từ đó có trong bài đánh giá\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents] # Tạo danh sách featuresets chứa các cặp (từ điển features, thể loại) cho tất cả các bài đánh giá \n",
    "len(featuresets) # Hiển thị số lượng các cặp (từ điển features, thể loại) trong danh sách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1500:], featuresets[:500] \n",
    "# Chia danh sách featuresets thành hai tập: tập huấn luyện train_set chứa các cặp từ chỉ mục 1500 trở đi và tập kiểm tra test_set chứa các cặp từ chỉ mục 0 đến 499\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # Huấn luyện Naive Bayes với tập huấn luyện train_set\n",
    "print(nltk.classify.accuracy(classifier, test_set)) # Tính toán và hiển thị độ chính xác của bộ phân loại trên tập kiểm tra test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             wonderfully = True              pos : neg    =     16.4 : 1.0\n",
      "                terrific = True              pos : neg    =     13.6 : 1.0\n",
      "                  random = True              neg : pos    =      9.9 : 1.0\n",
      "                  dennis = True              neg : pos    =      9.2 : 1.0\n",
      "           uninteresting = True              neg : pos    =      9.2 : 1.0\n",
      "                develops = True              pos : neg    =      8.0 : 1.0\n",
      "                  raised = True              pos : neg    =      8.0 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.9 : 1.0\n",
      "                  finest = True              pos : neg    =      7.8 : 1.0\n",
      "                  agrees = True              pos : neg    =      7.3 : 1.0\n",
      "                 balance = True              pos : neg    =      7.3 : 1.0\n",
      "                  shines = True              pos : neg    =      7.3 : 1.0\n",
      "                   views = True              pos : neg    =      7.3 : 1.0\n",
      "                    skip = True              neg : pos    =      7.3 : 1.0\n",
      "                  poorly = True              neg : pos    =      7.2 : 1.0\n",
      "              ridiculous = True              neg : pos    =      6.9 : 1.0\n",
      "                  allows = True              pos : neg    =      6.7 : 1.0\n",
      "                   inane = True              neg : pos    =      6.7 : 1.0\n",
      "                lifeless = True              neg : pos    =      6.7 : 1.0\n",
      "                nonsense = True              neg : pos    =      6.7 : 1.0\n",
      "                    alex = True              pos : neg    =      6.6 : 1.0\n",
      "                   candy = True              pos : neg    =      6.6 : 1.0\n",
      "                  deeply = True              pos : neg    =      6.6 : 1.0\n",
      "                   shape = True              pos : neg    =      6.6 : 1.0\n",
      "                  status = True              pos : neg    =      6.6 : 1.0\n",
      "               fantastic = True              pos : neg    =      6.5 : 1.0\n",
      "                    tony = True              pos : neg    =      6.5 : 1.0\n",
      "                   awful = True              neg : pos    =      6.4 : 1.0\n",
      "              impression = True              neg : pos    =      6.3 : 1.0\n",
      "               cardboard = True              neg : pos    =      6.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30) # Hiển thị 30 đặc trưng thông tin nhất mà bộ phân loại sử dụng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
